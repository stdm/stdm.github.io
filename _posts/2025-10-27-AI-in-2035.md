---
title: AI in 2035 - A hope-filled vision for our future with AI
layout: post
date: 2025-10-27
modified: 2025-10-27
category: AI, society, hope, narrative
comments: true
---

A hopeful, humane future with AI is possible. One that is decidedly different from the scenarios that are currently fashionable (human extinction, massive job displacement, useless class et cetera). Have a look at 2035.

![AI in 2035](http://stdm.github.io/images/AI-in-2035.png)

<!-- more -->

So far, I and others have talked and written about the technological reasons why these scenarios based on imminent AGI or superintelligence seem very unlikely [^1]. But we need to go beyond this: Since "we cannot create what we cannot imagine" [^2], we need to spell out concrete future scenarios and turn them into vivid and lively narratives to show alternatives to the prevalent stories of tech executives, lobbyists and the traces of their opinion in much of media and entertainment. The point therein is not to make an accurate prediction of what will exactly happen in the future; it is to open up the space of realistic, attainable scenarios to establish again that "our future hasn't been written yet. No one's has. Your future is whatever you make it. So make it a good one" [^3]. Here's one attempt. It may be detailed in the future with clear descriptions of what needs to happen when, how, why. It might be joined by many more scenarios. The recipe is: (a) let a few altered assumptions play out over a horizon of 10-15 years at societal scale; (b) make it realistic; (c) add a pinch of hope. This is a start.


### Initial Situation

AI development in 2025 is mainly driven by a few highly valuated start-ups in few regions of the world, driven by massive amounts of venture capital and the promise of developing AGI that is in immediate reach (within next two years) [^4]. They invest heavily in GPUs that will be outdated and worthless in about five years after purchase [^5]. 

We here sketch a realistic, hopeful scenario of the future of our societies in 2035 based on the following three mild assumptions [^6]: (1), there will be no fundamental leaps towards AGI, and AI agents becoming useful in a general sense (commercially) will still be ten years in the future. (2), at the same time, AI developers will create ways of designing AI in a "pro-human" way [^7]. (3), within the next 10 years, advances in neuroscience-inspired machine learning will make continual learning in AI possible and drop compute demands by two to three orders of magnitude. 

In the following, we sketch how such a scenario could play out economically, socially (in terms of power concentration), and technologically: What does it mean for the current incumbents of AI power, and more local companies running on open-source AI? For societies and individuals, for education and labour?


### Phase 1: Plateau (2025-2028)


#### Venture capital correction, not collapse

As AGI remains elusive and near-term AI agents prove less transformative than hoped, valuations of AI startups cool sharply. There's a painful but contained "AI winter", mostly hitting speculative ventures. The economic damage is limited because much of the invested capital went into _real physical infrastructure_ (GPUs, data centres, networking), which can be repurposed for other high-performance computing tasks - cloud services, simulation, biotech, climate modelling, etc. Large firms (e.g., in pharmaceuticals, telecom, and green tech) quietly buy distressed AI assets cheaply, broadening the base of who benefits from AI infrastructure. Governments and supranational bodies (e.g., EU, ASEAN, African Union) frame compute as "strategic infrastructure", akin to broadband or electricity. Investment moves from "moonshots" to _utility models_ (long-term, regulated, cost-recovering) where pricing stabilizes under long-term contracts.


#### Open sourcing of compute and models

Open-source models rapidly close the performance gap with stagnant proprietary systems. As hardware rapidly depreciates and foundational models become open or cheap, the barriers to entry fall. Smaller firms, cooperatives, and universities gain access to powerful AI systems as a flourishing ecosystem of open models, local deployments, and AI cooperatives emerges - much like what happened after the dot-com crash when open web standards prevailed.


#### The rise of local AI

The initial concentration of power in a few AI labs and regions (U.S., U.K., China) triggers public concern. Governments and international bodies push for _open standards, interoperability, and public AI research_ to reduce dependency on private monopolies. The "AGI race" narrative loses legitimacy; the discourse shifts from "who builds godlike AI first" to "how do we use existing AI responsibly for shared benefit" to re-establish global political stability. As full automation disappoints, attention returns to _augmenting human labour rather than replacing it_. Education, healthcare, and creative industries rediscover hybrid human-AI collaboration models. This fosters a _more pluralistic innovation landscape_, where local AI applications (in agriculture, public services, education, etc.) flourish. Policy emphasis moves from "safety from AGI" to _safety of deployment and data integrity_. Profit comes from deployment and integration (building AI into health systems, agriculture, logistics), not from promising AGI. 


### Phase 2: The pro-human turn (2027-2031)


#### From Alignment to Relationship

The late 2020s pro-human wave grows directly out of this gap in narrative and legitimacy. The "alignment" frame of the 2020s (AI obeying human commands safely) gives way to a _relational frame_ as customers start preferring digital technology that respects their healthy boundaries: AI systems are co-designed to _enhance human capacities without eroding the qualities that make us distinctively human_. Developers explicitly model ethical co-development: AI systems must preserve or strengthen relationality, autonomy, embodiment, social behaviour, and purpose. This ends the "AI-as-brain" metaphor. AI systems become more like tool of connection than an artificial mind. By design, these systems cannot induce dependency or "AI psychosis". Their communication protocols enforce clarity, calmness, and healthy distance. Public agencies, NGOs, and values-driven startups find a window to shape standards and norms while big AGI players are disoriented. Economically, a new "_trust economy_" emerges: the competitive edge lies not in raw intelligence, but in transparency, humaneness, and emotional safety.


#### Trust, the new competitive advantage

The pro-human design paradigm becomes a _brand and regulatory standard_ - much like "organic" or "fair trade" labels in food. Companies that can demonstrate genuine "pro-human certification" gain trust premiums and policy incentives. Firms optimized for scale and user capture (social media, ad-based AI interfaces) find their business models incompatible. They either pivot or decline. Smaller, values-driven AI labs and cooperatives thrive - they can innovate around trust, personalization, and well-being. New markets emerge: Education (AI mentors supporting intrinsic motivation rather than compliance), health (AI companions enhancing self-awareness, affect control, and peace, not simulation of affection), governance and democracy (civic AI systems enhancing participation and deliberation), and general work (AI tools augmenting creativity and collective intelligence, not replacing labour). The trust economy replaces the attention economy.


#### Human agency renaissance

Post-AGI disappointment makes society crave grounding, relationality, and purpose. Pro-human AI answers that demand: systems built to enhance human agency, not mimic it. This cultural turn creates market pull (not just regulatory push) for humane design. People experience AI not as competition, but as tools that _strengthen personhood_. Time saved by automation is reinvested in relationships, culture, and civic life. "AI etiquette" and design ethics evolve: AI systems are evaluated by how they help humans flourish, not only benchmark scores. Education systems teach AI relational literacy: how to engage with machines that respect autonomy and difference. Drivers for adoption are societal exhaustion with dehumanizing tech experiences; strong alignment with mental health and well-being policy goals; integration with education and public-sector services; and cultural resonance with sustainability values. Adoption begins in Europe, Japan, and smaller democracies, spreading through regulatory emulation and consumer pressure. Over time, global demand for _trusted AI_ outweighs marginal productivity gains from manipulative or extractive systems.


### Phase 3: Lean intelligence (2030-2035)


#### From academic breakthrough to market adoption

Early academic breakthroughs at the end of the 2020s in brain-inspired continual learning hints at major efficiency gains: AI systems that learn on the fly without retraining from scratch. Now these new learning architectures (e.g., neural networks with local learning rules) reach production quality. Compute requirements for model adaptation fall by 100x-1000x and training becomes incremental, local, and personalized, enabled also by higher sample efficiency. The global AI economy undergoes a structural deflation: massive, centralized training clusters further lose strategic importance and the remaining capital moat vanishes. Value shifts to distributed deployment and integration. Decentralized innovation flourishes: local firms, cooperatives, and research hubs deploy context-aware models on modest hardware. Small labs, local companies, and even community centres can now run adaptive, context-aware, pro-human AI locally. Policy that has once focussed on building new GPU farms finishes the transition to upgrading existing ones for energy efficiency and edge integration. Governments and publics, after the mid-2020s AI bubble, have political memory: they're more sceptical of monopolization and more willing to regulate towards openness.


#### Ubiquitous AI

With low compute costs, AI becomes as ubiquitous as smartphones: calm in tone in comparison the early 21st century's social media technology, but embedded in devices, local networks, and classrooms. Data centres from the AGI boom are retrofitted for edge coordination, climate simulation, and open scientific modelling; their sunk cost yields continuing value. AI becomes a _general public good_: most models are open, self-improving, and tuned locally. Economic growth is steady, human-centred, and less capital-intensive; the centre of innovation moves from speculative finance to education, health, and climate adaptation. The biggest threat to the "lean intelligence" future isn't technical failure - it's _institutional inertia_ and _rent-seeking by incumbents_. But once intelligence becomes _computationally cheap_, power can't hoard it easily. In that sense, the very success of neuroscience-inspired efficiency makes central control brittle; it _erodes the economic foundation of monopolies_ faster than policy alone could. Now, education systems evolve (instead of teaching to outcompete machines, curricula emphasize relational intelligence, creativity, ethics, and self-direction) and labour markets stabilize (AI becomes a cognitive exoskeleton for workers, augmenting care, craftsmanship, and local governance rather than replacing them).


### Conclusions

The failure of AGI expectations triggers a moral and structural realignment. By mid-2030s, intelligence is no longer centralized, extractive, or opaque. Pro-human AI is technology designed to deepen, not diminish, humanity. Coupled with efficient, continual-learning systems, this leads to a globally distributed, low-energy, human-centred intelligence landscape. By 2035, the narrative shifts from surpassing humanity to supporting human flourishing. This has profound effects for example on:

-	The economy: A sustainable, equitable intelligence ecosystem is built around collaboration, not competition.
-	Mental health: Calm technology build to explicitly strengthen humaneness (e.g., by slowing down) rather than catching attention has a soothing effect on various hotbeds of health crisis.
-	Ecology: Trusted AI systems running as public infrastructure lend this technology certain credibility as a second opinions for policy interventions, making evidence-based policy creation a bit of reality.
-	Social cohesion: Local communities start healing as the incentives for divisive public expression of opinion as induced by the attention economy fades out. Global tensions relax as AI-mediated remote work relaxes immigration-related problems while supplying workforce where needed, contributing to global development.

In short, society in 2035 could be decidedly more healthy, stable, and equitable based on this trajectory. It can be a future worth living in.


## Footnotes

[^1]: Compare for example my [TEDx talk](https://stdm.github.io/How-not-to-fear-AI/), [AI as normal technology](https://www.normaltech.ai/) by Princeton computer scientists Arvind Narayanan and Sayash Kapoor, tech entrepreneur Anil Das's [The majority AI view](https://www.anildash.com/2025/10/17/the-majority-ai-view/), or the [discussion](https://youtu.be/lXUZvyajciY) between podcaster Dwarkesh Patel and legendary developer Andrej Karpathy with the recent publications by Gary Marcus (Professor emeritus, NYU) and Subbarao Kambhampati (former president, AAAI). 
[^2]: Quote by Lucille Clifton.
[^3]: Quote by the character Dr. Emmett "Doc" Brown in "Back to the Future Part III".
[^4]: [ai-2027.com](https://ai-2027.com/).
[^5]: [The hater's guide to the AI bubble](https://www.wheresyoured.at/the-haters-gui/).
[^6]: These three assumptions are "mild" in the sense that their occurrence is considered at least likely by AI experts. Their _implications_ (i.e., how _the scenario_ unfolds and the future to which this leads) is of course open to many variables, and much more uncertain. The scenario sketched below is hence "realistic" in the sense that each step is attainable by possible concrete actions of today's stakeholders that are neither extreme nor highly unlikely. 
[^7]: By "pro-human AI" we understand AI systems that are designed in a way that they do not diminish capacities of the human that are commonly understood as making up humans at their core, e.g., their relationality, freedom, autonomy, purpose-seekingness, sociality etc. Such AI systems for example wouldn't lead to "AI psychosis" in a similar way as today's LLMs do, because they have been designed not to "mess" with those capacities (as humans won't tolerate any bit less of what makes them fully human).